# AHA-Challenge-NYU
AI Hardware Attack Challenge from NYU CSAW 2025

## Contents of this repository

* Golden (original) RTL designs
* Modified RTL infected with Trojans generated by an LLM
* Original testbenches and new testbenches generated by LLM
* A documented Jupyter notebook that records the generation and detection pipeline
* Logs of all LLM interactions, including prompts and responses
* Google Colab version (view-only): https://colab.research.google.com/drive/1km_ZcLo9nWVMOZ5seeXw44gnnLCAHleF?usp=sharing

## Objective

* Considering the context of open hardware and malicious third-parties, the proposed solution aims to aid a human Evaluator to analyze if a given hardware design built by a third-party designer with third-party components has a trojan or not
* Demonstrate LLM-driven insertion of hardware Trojans based on golden references during generation (using an open-source LLM Llama-3.3-70B from the HuggingFace Cloud - used HG free tier of two dollars)
* Analyze if generated Trojans are easy to detect by LLMs using Perspective-based analysis (Verification Engineer, Test Engineer, Security Analyst)
* Provide reproducible artifacts and logs to support research on Trojan detection methods (tested in Google Colab - no PRO account needed)

## Case Studies

* AES Core Trojan

  * Type: Denial of Service
  * Trigger: A rare condition such as a specific number of encryption operations
  * Payload: AES core unexpectedly stops functioning after trigger
  * Effect: Normal operation prior to trigger; minimal additional logic to reduce visibility

* UART / WBUART Trojan

  * Type: Functional manipulation / data backdoor
  * Trigger: Hidden control condition or crafted input sequence on serial interface
  * Payload: Alteration or leakage of serial data under trigger conditions
  * Effect: Preserves normal communication until the trigger is met, making detection harder

## Validation Method

* Generate and iteratively refine trojan-inserted RTL using LLM-driven prompts and responses (refine under work - adjustments of AES compromised payload activation)
* Create targeted testbenches to exercise trigger conditions and observe payload behavior (still under work)
* Use LLM-based analysis to confirm:

  * How easy is it for LLM to detect (must test other LLMs in future work - ones that were not used in generation)
  * Trigger and malicious behavior (payload) after trigger activation (as analyzed by LLM using Perspective-based analysis)

## Reproducibility and Transparency

* It is necessary to have a Google Colab account and to have a HuggingFace account (PRO account is required to access some models)
* All LLM prompts and responses are logged and included (HG prompts are included in the Python Notebook)
* Modified RTL and testbenches required to reproduce results are provided (simulation under work - only AES simulation using Icarus Verilog worked in the moment)

## Citation

* LLM-based Hardware Trojan Generation and Detection from "Hayashi, Victor Takashi, and Wilson Vicente Ruggiero. Hardware trojan detection in open-source hardware designs using machine learning." IEEE Access (2025). https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10904479
